{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Review\n",
    "## Business Problem\n",
    "In today’s information era, even non technological\n",
    "savvy consumers can quickly discover information\n",
    "about a business. This is great for customers, as it\n",
    "allows them to identify the “snake oil salesmen” (this\n",
    "is a metaphor that was coined for individuals who sold\n",
    "fraudulent goods). The motivation behind this project\n",
    "is to develop a tool that can show businesses, in this\n",
    "case restaurants, what aspects of their business are\n",
    "leading to their reviews without them having to crawl\n",
    "through potentially hundreds of text data. Is it their\n",
    "price? Their food? Their service? Businesses that know\n",
    "this information can get ahead of the competition by\n",
    "improving the areas that lead to less positive reviews.\n",
    "For example, a restaurant that knows they possess four\n",
    "stars on Yelp, along with a breakdown of which\n",
    "aspects of their establishment contributed to these four\n",
    "stars might see that they lose marks on their service.\n",
    "This may encourage management to retrain\n",
    "waiters/waitresses in an effort to improve their service\n",
    "\n",
    "score and thus their overall Yelp score. This prospect\n",
    "would be extremely useful to restaurants because when\n",
    "was the last time the reader went to a restaurant\n",
    "without reading some online reviews beforehand?\n",
    "\n",
    "## The Data\n",
    "The data, as discussed above, is text review data. In this re-enforcement learning stage, the data set comes from two locations. One set is a snapshot of about 10,000 records from a larger database where the trained model will be deployed on. This snapshot was manually coded with one of the following categories\n",
    "* Food#Postive\n",
    "* Food#Negative\n",
    "* Price#Postive\n",
    "* Price#Negative\n",
    "* Quality#Positive\n",
    "* Quality#Negative\n",
    "* Restaurant#Positive\n",
    "* Restaurant#Negative\n",
    "* Location#Postive\n",
    "* Location#Negative\n",
    "* Service#Positive\n",
    "* Service#Negative\n",
    "This was done for validation purposes. The other 1,600 records come from a Yelp Kaggle competition. The categories were coded to reflect categories above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1 Read in data\n",
    "# Step 2 Preprocess text data\n",
    "# Step 3 Word Embedding\n",
    "# Step 4 Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "import re\n",
    "# importing keras packages\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "from keras.layers import Flatten\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential #### required layer in our LSTM network\n",
    "from keras.layers import Dense #### required layer in our LSTM network\n",
    " #### required layer in our LSTM network\n",
    "from keras.layers.embeddings import Embedding #### required layer in our LSTM network\n",
    "from keras.preprocessing import sequence #### Packaged preprocessing step in Keras\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yelp=pd.read_excel('Sample_Yelp_Ver2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            category                                               text\n",
      "0      FOOD#POSITIVE  There are so many options to choose from, what...\n",
      "1  LOCATION#NEGATIVE  To save a little time, we opted to sit outside...\n",
      "2   QUALITY#NEGATIVE     MISSES - Risotto: one bite was enough for me.\"\n",
      "3      FOOD#POSITIVE  I really loved the different and inovated touc...\n",
      "4  LOCATION#POSITIVE  The view of the Bellagio fountain from this pl...\n"
     ]
    }
   ],
   "source": [
    "print(yelp.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analytics\n",
    "Text analytics means examining text that was written by, or about, customers. You find patterns and topics of interest, and then take practical action based on what you learn.\n",
    "\n",
    "Text analytics can be performed manually, but it is an inefficient process. Therefore, text analytics software has been created that uses text mining and natural language processing algorithms to find meaning in huge amounts of text, which is the attempt of this project.\n",
    "\n",
    "### PreProcessing\n",
    "Unlike regular data preprocessing steps, text mining requires an alternate approach. The end goal is more or less the same, a normalised data set that is numbers only containing less noise. \n",
    "\n",
    "#### Lower Case \n",
    "This is a step that helps put the text on an equal footing. The most simple way to explain it is as follows: One wouldn't count *Everything* and *everything* as two separate words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PreProcessing\n",
    "#step 1 lower case\n",
    "#step 2 punctuation\n",
    "#step 3 stop word\n",
    "#step 4 common word removal\n",
    "#step 5 rare word removal\n",
    "#step 6 token\n",
    "#step 7 stemming\n",
    "#step 8 lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    there are so many options to choose from, what...\n",
       "1    to save a little time, we opted to sit outside...\n",
       "2       misses - risotto: one bite was enough for me.\"\n",
       "3    i really loved the different and inovated touc...\n",
       "4    the view of the bellagio fountain from this pl...\n",
       "Name: lower, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step 1\n",
    "yelp['lower'] = yelp.text.apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "yelp.lower.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Punctuation\n",
    "Again, this is just to keep things equal and prevent items like full stops from adding noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    there are so many options to choose from whate...\n",
       "1    to save a little time we opted to sit outside ...\n",
       "2            misses risotto one bite was enough for me\n",
       "3    i really loved the different and inovated touc...\n",
       "4    the view of the bellagio fountain from this pl...\n",
       "Name: no_punc, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step 2\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "reg_tok = RegexpTokenizer(r'\\w+')#+ is one or more\n",
    "yelp['no_punc'] = yelp['lower'].apply(lambda x: ' '.join(reg_tok.tokenize(x)))\n",
    "yelp.no_punc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop Words\n",
    "Stop words are those words which are filtered out before further processing of text, since these words contribute little to overall meaning, given that they are generally the most common words in a language. For instance, \"the,\" \"and,\" and \"a,\" while all required words in a particular passage, don't generally contribute greatly to one's understanding of content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    many options choose whatever like good chance ...\n",
       "1    save little time opted sit outside 110 degree ...\n",
       "2                       misses risotto one bite enough\n",
       "3    really loved different inovated touch cheff gi...\n",
       "4             view bellagio fountain place really nice\n",
       "Name: no_stop, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step 3\n",
    "yelp['no_stop'] = yelp['no_punc'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "yelp.no_stop.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common and Rare Words\n",
    "Removing the most common and the rarest words often can add a small amount of value to your model. This is because words like \"good\" or \"food\", often don't add a specific value to a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "good          42\n",
       "buffet        39\n",
       "food          30\n",
       "great         27\n",
       "one           21\n",
       "service       20\n",
       "vegas         20\n",
       "go            19\n",
       "place         19\n",
       "really        19\n",
       "restaurant    18\n",
       "time          17\n",
       "best          16\n",
       "like          15\n",
       "people        14\n",
       "want          14\n",
       "get           13\n",
       "bellagio      13\n",
       "got           13\n",
       "try           13\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = pd.Series(' '.join(yelp['no_stop']).split()).value_counts()[:20]#combining all rows and then splitting and converitign and value count\n",
    "freq\n",
    "#looking at these, we actually want to keep them so no need to carry out this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#step 4\n",
    "rare = pd.Series(' '.join(yelp['no_stop']).split()).value_counts()[-600:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#step 5\n",
    "rare = list(rare.index)\n",
    "yelp['no_rare'] = yelp['no_stop'].apply(lambda x: \" \".join(x for x in x.split() if x not in rare))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    many option choose whatever like good chance f...\n",
       "1    save little time often sit outside 110 degree ...\n",
       "2                       misses risotto one bite enough\n",
       "3    really loved different invited touch chef give...\n",
       "4             view bellagio fountain place really nice\n",
       "Name: no_stop, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just did for note\n",
    "from textblob import TextBlob\n",
    "# not really doing that for tutorial, this is just demo of it\n",
    "yelp['no_stop'][:5].apply(lambda x: str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenising\n",
    "Tokenization is a step which splits longer strings of text into smaller pieces, or tokens. Larger chunks of text can be tokenized into sentences, sentences can be tokenized into words, etc. Further processing is generally performed after a piece of text has been appropriately tokenized. Tokenization is also referred to as text segmentation or lexical analysis. Sometimes segmentation is used to refer to the breakdown of a large chunk of text into pieces larger than words (e.g. paragraphs or sentences), while tokenization is reserved for the breakdown process which results exclusively in words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         many options choose like good chance finding\n",
       "1    save little time opted sit outside 110 vegas l...\n",
       "2                                    misses one enough\n",
       "3                          really loved different food\n",
       "4             view bellagio fountain place really nice\n",
       "Name: token, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step 6\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "_word_tokenize = TreebankWordTokenizer()\n",
    "yelp['token'] = yelp['no_rare'].apply(lambda x: ' '.join(_word_tokenize.tokenize(x)))\n",
    "yelp.token.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "Stemming is the process of eliminating affixes (suffixed, prefixes, infixes, circumfixes) from a word in order to obtain a word stem. For example, running -> run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               mani option choos like good chanc find\n",
       "1    save littl time opt sit outsid 110 vega littl ...\n",
       "2                                      miss one enough\n",
       "3                              realli love differ food\n",
       "4             view bellagio fountain place realli nice\n",
       "Name: stemed, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step 7\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "st = SnowballStemmer(\"english\")\n",
    "yelp['stemed']=yelp['token'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "yelp.stemed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Lemmatization is related to stemming, differing in that lemmatization is able to capture canonical forms based on a word's lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                mani option choos like good chanc find\n",
       "1     save littl time opt sit outsid 110 vega littl ...\n",
       "2                                       miss one enough\n",
       "3                               realli love differ food\n",
       "4              view bellagio fountain place realli nice\n",
       "5     realli quit simpl differ name peopl outsid yel...\n",
       "6                  look beef like crap check make place\n",
       "7     good view fountain bellagio even sit insid lik...\n",
       "8                          would high recommend restaur\n",
       "9                  disappoint unimpress present qualiti\n",
       "10                       give 4 star minus 1 locat nois\n",
       "11                               bad locat especi night\n",
       "12                 price cheap servic cool place open 7\n",
       "13                  server cool quick pace attitud meal\n",
       "14    want tri someth tri buffet bellagio pleasant s...\n",
       "15                                          place clean\n",
       "16                          staff came clear tabl check\n",
       "17    per person breakfast brunch thought buffet bar...\n",
       "18    salad item good everyon el said sit patio see ...\n",
       "19                       never like coffe flavor fluffi\n",
       "Name: lemma, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step 8\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "yelp['lemma']=yelp['stemed'].apply(lambda x: \" \".join([wordnet_lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "yelp.lemma.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding \n",
    "Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with one dimension per word to a continuous vector space with a much lower dimension.\n",
    "\n",
    "Methods to generate this mapping include neural networks,[1] dimensionality reduction on the word co-occurrence matrix, probabilistic models,explainable knowledge base method,and explicit representation in terms of the context in which words appear. They are needed as computers do not understand words, and in laymans terms convert words to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepping the Word Embedding by getting dictionary length and max sentence length\n",
    "\n",
    "yelp.lemma.str.len().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "count=Counter(\" \".join(yelp.lemma).split(\" \")).items()\n",
    "# print(sorted(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "546"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#length of dictionary\n",
    "len(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go back vega 18 day probabl eat least 3 time entir staff girl front manag busser chef alway incred one brought told even want burger go dessert\n"
     ]
    }
   ],
   "source": [
    "#longest sentence\n",
    "print(max(yelp.lemma, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# embeddings = tf.Variable(\n",
    "#     tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "# embed = tf.nn.embedding_lookup(embeddings, train_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3485 5715 3871 ...    0    0    0]\n",
      " [ 362 1220 3682 ...    0    0    0]\n",
      " [1321 4691 1402 ...    0    0    0]\n",
      " ...\n",
      " [1924 2579 3050 ...    0    0    0]\n",
      " [4621  593 4812 ...    0    0    0]\n",
      " [1484 3643 2692 ...    0    0    0]]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 130, 32)           192000    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                49932     \n",
      "=================================================================\n",
      "Total params: 241,932\n",
      "Trainable params: 241,932\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 252 samples, validate on 63 samples\n",
      "Epoch 1/30\n",
      "252/252 [==============================] - 2s 8ms/step - loss: 2.4616 - acc: 0.1111 - val_loss: 2.4016 - val_acc: 0.1905\n",
      "Epoch 2/30\n",
      "252/252 [==============================] - 0s 235us/step - loss: 2.4026 - acc: 0.1548 - val_loss: 2.3566 - val_acc: 0.1905\n",
      "Epoch 3/30\n",
      "252/252 [==============================] - 0s 231us/step - loss: 2.3752 - acc: 0.1548 - val_loss: 2.3581 - val_acc: 0.1905\n",
      "Epoch 4/30\n",
      "252/252 [==============================] - 0s 202us/step - loss: 2.3680 - acc: 0.1548 - val_loss: 2.3778 - val_acc: 0.1905\n",
      "Epoch 5/30\n",
      "252/252 [==============================] - 0s 287us/step - loss: 2.3680 - acc: 0.1548 - val_loss: 2.3828 - val_acc: 0.1905\n",
      "Epoch 6/30\n",
      "252/252 [==============================] - 0s 214us/step - loss: 2.3615 - acc: 0.1548 - val_loss: 2.3835 - val_acc: 0.1905\n",
      "Epoch 7/30\n",
      "252/252 [==============================] - 0s 265us/step - loss: 2.3553 - acc: 0.1548 - val_loss: 2.3831 - val_acc: 0.1905\n",
      "Epoch 8/30\n",
      "252/252 [==============================] - 0s 301us/step - loss: 2.3495 - acc: 0.1548 - val_loss: 2.3782 - val_acc: 0.1905\n",
      "Epoch 9/30\n",
      "252/252 [==============================] - 0s 312us/step - loss: 2.3447 - acc: 0.1548 - val_loss: 2.3773 - val_acc: 0.1905\n",
      "Epoch 10/30\n",
      "252/252 [==============================] - 0s 182us/step - loss: 2.3401 - acc: 0.1548 - val_loss: 2.3733 - val_acc: 0.1905\n",
      "Epoch 11/30\n",
      "252/252 [==============================] - 0s 207us/step - loss: 2.3329 - acc: 0.1627 - val_loss: 2.3704 - val_acc: 0.1905\n",
      "Epoch 12/30\n",
      "252/252 [==============================] - 0s 249us/step - loss: 2.3250 - acc: 0.1667 - val_loss: 2.3707 - val_acc: 0.1905\n",
      "Epoch 13/30\n",
      "252/252 [==============================] - 0s 245us/step - loss: 2.3167 - acc: 0.1667 - val_loss: 2.3695 - val_acc: 0.1905\n",
      "Epoch 14/30\n",
      "252/252 [==============================] - 0s 186us/step - loss: 2.3084 - acc: 0.1825 - val_loss: 2.3721 - val_acc: 0.1905\n",
      "Epoch 15/30\n",
      "252/252 [==============================] - 0s 222us/step - loss: 2.2968 - acc: 0.2063 - val_loss: 2.3674 - val_acc: 0.1905\n",
      "Epoch 16/30\n",
      "252/252 [==============================] - 0s 208us/step - loss: 2.2842 - acc: 0.2103 - val_loss: 2.3626 - val_acc: 0.1905\n",
      "Epoch 17/30\n",
      "252/252 [==============================] - 0s 178us/step - loss: 2.2687 - acc: 0.2222 - val_loss: 2.3646 - val_acc: 0.1905\n",
      "Epoch 18/30\n",
      "252/252 [==============================] - 0s 194us/step - loss: 2.2512 - acc: 0.2500 - val_loss: 2.3655 - val_acc: 0.1905\n",
      "Epoch 19/30\n",
      "252/252 [==============================] - 0s 186us/step - loss: 2.2312 - acc: 0.2738 - val_loss: 2.3653 - val_acc: 0.2063\n",
      "Epoch 20/30\n",
      "252/252 [==============================] - 0s 194us/step - loss: 2.2066 - acc: 0.3333 - val_loss: 2.3657 - val_acc: 0.1746\n",
      "Epoch 21/30\n",
      "252/252 [==============================] - 0s 210us/step - loss: 2.1760 - acc: 0.3254 - val_loss: 2.3588 - val_acc: 0.1746\n",
      "Epoch 22/30\n",
      "252/252 [==============================] - 0s 190us/step - loss: 2.1380 - acc: 0.3452 - val_loss: 2.3607 - val_acc: 0.1587\n",
      "Epoch 23/30\n",
      "252/252 [==============================] - 0s 186us/step - loss: 2.0917 - acc: 0.3849 - val_loss: 2.3552 - val_acc: 0.1746\n",
      "Epoch 24/30\n",
      "252/252 [==============================] - 0s 189us/step - loss: 2.0364 - acc: 0.4127 - val_loss: 2.3645 - val_acc: 0.1587\n",
      "Epoch 25/30\n",
      "252/252 [==============================] - 0s 253us/step - loss: 1.9671 - acc: 0.5040 - val_loss: 2.3520 - val_acc: 0.1587\n",
      "Epoch 26/30\n",
      "252/252 [==============================] - 0s 259us/step - loss: 1.8967 - acc: 0.5159 - val_loss: 2.3410 - val_acc: 0.1746\n",
      "Epoch 27/30\n",
      "252/252 [==============================] - 0s 245us/step - loss: 1.8330 - acc: 0.5317 - val_loss: 2.3524 - val_acc: 0.1746\n",
      "Epoch 28/30\n",
      "252/252 [==============================] - 0s 211us/step - loss: 1.7604 - acc: 0.6310 - val_loss: 2.3369 - val_acc: 0.1746\n",
      "Epoch 29/30\n",
      "252/252 [==============================] - 0s 182us/step - loss: 1.6933 - acc: 0.5873 - val_loss: 2.3614 - val_acc: 0.1587\n",
      "Epoch 30/30\n",
      "252/252 [==============================] - 0s 237us/step - loss: 1.6455 - acc: 0.6905 - val_loss: 2.3695 - val_acc: 0.2063\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x224625766a0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# define documents\n",
    "\n",
    "# define class labels\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(yelp.category)\n",
    "encoded_Y = encoder.transform(yelp.category)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "# integer encode the documents\n",
    "vocab_size = 6000\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in yelp.lemma]\n",
    "#print(encoded_docs)\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 130\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)\n",
    "# define the model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_size, 32, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(12, activation='sigmoid'))\n",
    "#model.add(Dropout(0.3))\n",
    "#model.add(Dense(12, activation='sigmoid'))\n",
    "#model.add(Dropout(0.3))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_docs,dummy_y,test_size=0.2)\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=30, batch_size=64)\n",
    "# evaluate the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3485 5715 3871 ...    0    0    0]\n",
      " [ 362 1220 3682 ...    0    0    0]\n",
      " [1321 4691 1402 ...    0    0    0]\n",
      " ...\n",
      " [1924 2579 3050 ...    0    0    0]\n",
      " [4621  593 4812 ...    0    0    0]\n",
      " [1484 3643 2692 ...    0    0    0]]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 130, 32)           192000    \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 12)                49932     \n",
      "=================================================================\n",
      "Total params: 241,932\n",
      "Trainable params: 241,932\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 252 samples, validate on 63 samples\n",
      "Epoch 1/100\n",
      "252/252 [==============================] - 0s 1ms/step - loss: 2.4686 - acc: 0.0952 - val_loss: 2.4222 - val_acc: 0.0794\n",
      "Epoch 2/100\n",
      "252/252 [==============================] - 0s 197us/step - loss: 2.4097 - acc: 0.1548 - val_loss: 2.3947 - val_acc: 0.0952\n",
      "Epoch 3/100\n",
      "252/252 [==============================] - 0s 218us/step - loss: 2.3824 - acc: 0.1587 - val_loss: 2.3908 - val_acc: 0.0952\n",
      "Epoch 4/100\n",
      "252/252 [==============================] - 0s 380us/step - loss: 2.3619 - acc: 0.2024 - val_loss: 2.4018 - val_acc: 0.1746\n",
      "Epoch 5/100\n",
      "252/252 [==============================] - 0s 218us/step - loss: 2.3601 - acc: 0.1984 - val_loss: 2.4108 - val_acc: 0.1746\n",
      "Epoch 6/100\n",
      "252/252 [==============================] - 0s 241us/step - loss: 2.3561 - acc: 0.1944 - val_loss: 2.4136 - val_acc: 0.1746\n",
      "Epoch 7/100\n",
      "252/252 [==============================] - 0s 231us/step - loss: 2.3519 - acc: 0.1984 - val_loss: 2.4072 - val_acc: 0.1746\n",
      "Epoch 8/100\n",
      "252/252 [==============================] - 0s 243us/step - loss: 2.3438 - acc: 0.2103 - val_loss: 2.4037 - val_acc: 0.1746\n",
      "Epoch 9/100\n",
      "252/252 [==============================] - 0s 244us/step - loss: 2.3384 - acc: 0.2103 - val_loss: 2.4031 - val_acc: 0.1905\n",
      "Epoch 10/100\n",
      "252/252 [==============================] - 0s 252us/step - loss: 2.3323 - acc: 0.2103 - val_loss: 2.4014 - val_acc: 0.1905\n",
      "Epoch 11/100\n",
      "252/252 [==============================] - 0s 275us/step - loss: 2.3257 - acc: 0.2103 - val_loss: 2.3994 - val_acc: 0.2063\n",
      "Epoch 12/100\n",
      "252/252 [==============================] - 0s 230us/step - loss: 2.3183 - acc: 0.2500 - val_loss: 2.3978 - val_acc: 0.1746\n",
      "Epoch 13/100\n",
      "252/252 [==============================] - 0s 234us/step - loss: 2.3097 - acc: 0.2500 - val_loss: 2.3956 - val_acc: 0.1746\n",
      "Epoch 14/100\n",
      "252/252 [==============================] - 0s 245us/step - loss: 2.3004 - acc: 0.2579 - val_loss: 2.3946 - val_acc: 0.1746\n",
      "Epoch 15/100\n",
      "252/252 [==============================] - 0s 222us/step - loss: 2.2883 - acc: 0.2659 - val_loss: 2.3950 - val_acc: 0.1746\n",
      "Epoch 16/100\n",
      "252/252 [==============================] - 0s 230us/step - loss: 2.2760 - acc: 0.2738 - val_loss: 2.3964 - val_acc: 0.1746\n",
      "Epoch 17/100\n",
      "252/252 [==============================] - 0s 211us/step - loss: 2.2625 - acc: 0.2738 - val_loss: 2.4042 - val_acc: 0.1746\n",
      "Epoch 18/100\n",
      "252/252 [==============================] - 0s 194us/step - loss: 2.2452 - acc: 0.2579 - val_loss: 2.4075 - val_acc: 0.1746\n",
      "Epoch 19/100\n",
      "252/252 [==============================] - 0s 179us/step - loss: 2.2240 - acc: 0.2698 - val_loss: 2.4001 - val_acc: 0.1746\n",
      "Epoch 20/100\n",
      "252/252 [==============================] - 0s 262us/step - loss: 2.2004 - acc: 0.2579 - val_loss: 2.3878 - val_acc: 0.1746\n",
      "Epoch 21/100\n",
      "252/252 [==============================] - 0s 197us/step - loss: 2.1714 - acc: 0.2659 - val_loss: 2.3810 - val_acc: 0.1746\n",
      "Epoch 22/100\n",
      "252/252 [==============================] - 0s 228us/step - loss: 2.1336 - acc: 0.3016 - val_loss: 2.3750 - val_acc: 0.1746\n",
      "Epoch 23/100\n",
      "252/252 [==============================] - 0s 266us/step - loss: 2.0869 - acc: 0.3333 - val_loss: 2.3844 - val_acc: 0.1746\n",
      "Epoch 24/100\n",
      "252/252 [==============================] - 0s 263us/step - loss: 2.0290 - acc: 0.4127 - val_loss: 2.3764 - val_acc: 0.1746\n",
      "Epoch 25/100\n",
      "252/252 [==============================] - 0s 222us/step - loss: 1.9680 - acc: 0.4484 - val_loss: 2.3768 - val_acc: 0.1746\n",
      "Epoch 26/100\n",
      "252/252 [==============================] - 0s 187us/step - loss: 1.8953 - acc: 0.5278 - val_loss: 2.3504 - val_acc: 0.2063\n",
      "Epoch 27/100\n",
      "252/252 [==============================] - 0s 210us/step - loss: 1.8268 - acc: 0.6468 - val_loss: 2.3678 - val_acc: 0.1746\n",
      "Epoch 28/100\n",
      "252/252 [==============================] - 0s 238us/step - loss: 1.7454 - acc: 0.5595 - val_loss: 2.3812 - val_acc: 0.1905\n",
      "Epoch 29/100\n",
      "252/252 [==============================] - 0s 245us/step - loss: 1.6733 - acc: 0.6389 - val_loss: 2.3644 - val_acc: 0.2063\n",
      "Epoch 30/100\n",
      "252/252 [==============================] - 0s 216us/step - loss: 1.5944 - acc: 0.7659 - val_loss: 2.3462 - val_acc: 0.2540\n",
      "Epoch 31/100\n",
      "252/252 [==============================] - 0s 238us/step - loss: 1.5406 - acc: 0.7579 - val_loss: 2.3415 - val_acc: 0.2222\n",
      "Epoch 32/100\n",
      "252/252 [==============================] - 0s 216us/step - loss: 1.4628 - acc: 0.7262 - val_loss: 2.3540 - val_acc: 0.1746\n",
      "Epoch 33/100\n",
      "252/252 [==============================] - 0s 267us/step - loss: 1.4002 - acc: 0.7619 - val_loss: 2.3706 - val_acc: 0.2063\n",
      "Epoch 34/100\n",
      "252/252 [==============================] - 0s 188us/step - loss: 1.3248 - acc: 0.8294 - val_loss: 2.3493 - val_acc: 0.2222\n",
      "Epoch 35/100\n",
      "252/252 [==============================] - 0s 205us/step - loss: 1.2638 - acc: 0.7976 - val_loss: 2.3298 - val_acc: 0.1905\n",
      "Epoch 36/100\n",
      "252/252 [==============================] - 0s 212us/step - loss: 1.2049 - acc: 0.8016 - val_loss: 2.3624 - val_acc: 0.2063\n",
      "Epoch 37/100\n",
      "252/252 [==============================] - 0s 202us/step - loss: 1.1342 - acc: 0.8492 - val_loss: 2.3325 - val_acc: 0.2540\n",
      "Epoch 38/100\n",
      "252/252 [==============================] - 0s 185us/step - loss: 1.0843 - acc: 0.8690 - val_loss: 2.3266 - val_acc: 0.2222\n",
      "Epoch 39/100\n",
      "252/252 [==============================] - 0s 240us/step - loss: 1.0299 - acc: 0.8571 - val_loss: 2.3559 - val_acc: 0.2222\n",
      "Epoch 40/100\n",
      "252/252 [==============================] - 0s 240us/step - loss: 0.9765 - acc: 0.8929 - val_loss: 2.3202 - val_acc: 0.2222\n",
      "Epoch 41/100\n",
      "252/252 [==============================] - 0s 261us/step - loss: 0.9257 - acc: 0.8770 - val_loss: 2.3122 - val_acc: 0.2540\n",
      "Epoch 42/100\n",
      "252/252 [==============================] - 0s 234us/step - loss: 0.8760 - acc: 0.9167 - val_loss: 2.3550 - val_acc: 0.2222\n",
      "Epoch 43/100\n",
      "252/252 [==============================] - 0s 192us/step - loss: 0.8382 - acc: 0.9246 - val_loss: 2.3366 - val_acc: 0.2222\n",
      "Epoch 44/100\n",
      "252/252 [==============================] - 0s 230us/step - loss: 0.7934 - acc: 0.9048 - val_loss: 2.3129 - val_acc: 0.2063\n",
      "Epoch 45/100\n",
      "252/252 [==============================] - 0s 198us/step - loss: 0.7517 - acc: 0.9405 - val_loss: 2.3108 - val_acc: 0.2540\n",
      "Epoch 46/100\n",
      "252/252 [==============================] - 0s 218us/step - loss: 0.7188 - acc: 0.9325 - val_loss: 2.3244 - val_acc: 0.2222\n",
      "Epoch 47/100\n",
      "252/252 [==============================] - 0s 247us/step - loss: 0.6806 - acc: 0.9365 - val_loss: 2.3340 - val_acc: 0.2222\n",
      "Epoch 48/100\n",
      "252/252 [==============================] - 0s 214us/step - loss: 0.6497 - acc: 0.9444 - val_loss: 2.3095 - val_acc: 0.2381\n",
      "Epoch 49/100\n",
      "252/252 [==============================] - 0s 226us/step - loss: 0.6182 - acc: 0.9603 - val_loss: 2.3207 - val_acc: 0.2540\n",
      "Epoch 50/100\n",
      "252/252 [==============================] - 0s 232us/step - loss: 0.5902 - acc: 0.9563 - val_loss: 2.3099 - val_acc: 0.2381\n",
      "Epoch 51/100\n",
      "252/252 [==============================] - 0s 198us/step - loss: 0.5632 - acc: 0.9484 - val_loss: 2.3329 - val_acc: 0.2222\n",
      "Epoch 52/100\n",
      "252/252 [==============================] - 0s 218us/step - loss: 0.5392 - acc: 0.9643 - val_loss: 2.3335 - val_acc: 0.2381\n",
      "Epoch 53/100\n",
      "252/252 [==============================] - 0s 198us/step - loss: 0.5144 - acc: 0.9683 - val_loss: 2.3083 - val_acc: 0.2381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/100\n",
      "252/252 [==============================] - 0s 198us/step - loss: 0.4954 - acc: 0.9603 - val_loss: 2.3306 - val_acc: 0.2222\n",
      "Epoch 55/100\n",
      "252/252 [==============================] - 0s 208us/step - loss: 0.4724 - acc: 0.9563 - val_loss: 2.3237 - val_acc: 0.2540\n",
      "Epoch 56/100\n",
      "252/252 [==============================] - 0s 200us/step - loss: 0.4530 - acc: 0.9643 - val_loss: 2.3365 - val_acc: 0.2381\n",
      "Epoch 57/100\n",
      "252/252 [==============================] - 0s 228us/step - loss: 0.4325 - acc: 0.9762 - val_loss: 2.3328 - val_acc: 0.2063\n",
      "Epoch 58/100\n",
      "252/252 [==============================] - 0s 289us/step - loss: 0.4140 - acc: 0.9603 - val_loss: 2.3261 - val_acc: 0.2222\n",
      "Epoch 59/100\n",
      "252/252 [==============================] - 0s 309us/step - loss: 0.3966 - acc: 0.9603 - val_loss: 2.3153 - val_acc: 0.2540\n",
      "Epoch 60/100\n",
      "252/252 [==============================] - 0s 272us/step - loss: 0.3808 - acc: 0.9683 - val_loss: 2.3355 - val_acc: 0.2540\n",
      "Epoch 61/100\n",
      "252/252 [==============================] - 0s 218us/step - loss: 0.3681 - acc: 0.9722 - val_loss: 2.3384 - val_acc: 0.2540\n",
      "Epoch 62/100\n",
      "252/252 [==============================] - 0s 277us/step - loss: 0.3508 - acc: 0.9683 - val_loss: 2.3501 - val_acc: 0.2540\n",
      "Epoch 63/100\n",
      "252/252 [==============================] - 0s 257us/step - loss: 0.3400 - acc: 0.9683 - val_loss: 2.3538 - val_acc: 0.2381\n",
      "Epoch 64/100\n",
      "252/252 [==============================] - 0s 257us/step - loss: 0.3241 - acc: 0.9683 - val_loss: 2.3180 - val_acc: 0.2540\n",
      "Epoch 65/100\n",
      "252/252 [==============================] - 0s 251us/step - loss: 0.3151 - acc: 0.9762 - val_loss: 2.3227 - val_acc: 0.2698\n",
      "Epoch 66/100\n",
      "252/252 [==============================] - 0s 261us/step - loss: 0.3015 - acc: 0.9762 - val_loss: 2.3541 - val_acc: 0.2222\n",
      "Epoch 67/100\n",
      "252/252 [==============================] - 0s 195us/step - loss: 0.2881 - acc: 0.9802 - val_loss: 2.3572 - val_acc: 0.2381\n",
      "Epoch 68/100\n",
      "252/252 [==============================] - 0s 252us/step - loss: 0.2785 - acc: 0.9802 - val_loss: 2.3442 - val_acc: 0.2698\n",
      "Epoch 69/100\n",
      "252/252 [==============================] - 0s 241us/step - loss: 0.2708 - acc: 0.9802 - val_loss: 2.3164 - val_acc: 0.2857\n",
      "Epoch 70/100\n",
      "252/252 [==============================] - 0s 255us/step - loss: 0.2598 - acc: 0.9722 - val_loss: 2.3600 - val_acc: 0.2381\n",
      "Epoch 71/100\n",
      "252/252 [==============================] - 0s 259us/step - loss: 0.2505 - acc: 0.9762 - val_loss: 2.3694 - val_acc: 0.2381\n",
      "Epoch 72/100\n",
      "252/252 [==============================] - 0s 226us/step - loss: 0.2422 - acc: 0.9762 - val_loss: 2.3482 - val_acc: 0.2381\n",
      "Epoch 73/100\n",
      "252/252 [==============================] - 0s 315us/step - loss: 0.2337 - acc: 0.9881 - val_loss: 2.3468 - val_acc: 0.2381\n",
      "Epoch 74/100\n",
      "252/252 [==============================] - 0s 284us/step - loss: 0.2249 - acc: 0.9921 - val_loss: 2.3538 - val_acc: 0.2540\n",
      "Epoch 75/100\n",
      "252/252 [==============================] - 0s 285us/step - loss: 0.2183 - acc: 0.9841 - val_loss: 2.3706 - val_acc: 0.2381\n",
      "Epoch 76/100\n",
      "252/252 [==============================] - 0s 272us/step - loss: 0.2106 - acc: 0.9802 - val_loss: 2.3717 - val_acc: 0.2381\n",
      "Epoch 77/100\n",
      "252/252 [==============================] - 0s 212us/step - loss: 0.2044 - acc: 0.9841 - val_loss: 2.3535 - val_acc: 0.2540\n",
      "Epoch 78/100\n",
      "252/252 [==============================] - 0s 224us/step - loss: 0.1983 - acc: 0.9881 - val_loss: 2.3635 - val_acc: 0.2540\n",
      "Epoch 79/100\n",
      "252/252 [==============================] - 0s 285us/step - loss: 0.1908 - acc: 0.9881 - val_loss: 2.3780 - val_acc: 0.2381\n",
      "Epoch 80/100\n",
      "252/252 [==============================] - 0s 297us/step - loss: 0.1864 - acc: 0.9881 - val_loss: 2.3927 - val_acc: 0.2222\n",
      "Epoch 81/100\n",
      "252/252 [==============================] - 0s 244us/step - loss: 0.1801 - acc: 0.9881 - val_loss: 2.3899 - val_acc: 0.2381\n",
      "Epoch 82/100\n",
      "252/252 [==============================] - 0s 249us/step - loss: 0.1742 - acc: 0.9881 - val_loss: 2.3824 - val_acc: 0.2540\n",
      "Epoch 83/100\n",
      "252/252 [==============================] - 0s 261us/step - loss: 0.1695 - acc: 0.9881 - val_loss: 2.3706 - val_acc: 0.2540\n",
      "Epoch 84/100\n",
      "252/252 [==============================] - 0s 200us/step - loss: 0.1650 - acc: 0.9881 - val_loss: 2.3810 - val_acc: 0.2381\n",
      "Epoch 85/100\n",
      "252/252 [==============================] - 0s 212us/step - loss: 0.1601 - acc: 0.9960 - val_loss: 2.3818 - val_acc: 0.2381\n",
      "Epoch 86/100\n",
      "252/252 [==============================] - 0s 237us/step - loss: 0.1552 - acc: 0.9921 - val_loss: 2.4055 - val_acc: 0.2381\n",
      "Epoch 87/100\n",
      "252/252 [==============================] - 0s 234us/step - loss: 0.1508 - acc: 0.9921 - val_loss: 2.4053 - val_acc: 0.2381\n",
      "Epoch 88/100\n",
      "252/252 [==============================] - 0s 210us/step - loss: 0.1472 - acc: 0.9881 - val_loss: 2.4154 - val_acc: 0.2381\n",
      "Epoch 89/100\n",
      "252/252 [==============================] - 0s 237us/step - loss: 0.1430 - acc: 0.9921 - val_loss: 2.4074 - val_acc: 0.2381\n",
      "Epoch 90/100\n",
      "252/252 [==============================] - 0s 212us/step - loss: 0.1387 - acc: 0.9881 - val_loss: 2.4005 - val_acc: 0.2540\n",
      "Epoch 91/100\n",
      "252/252 [==============================] - 0s 199us/step - loss: 0.1358 - acc: 0.9921 - val_loss: 2.3943 - val_acc: 0.2698\n",
      "Epoch 92/100\n",
      "252/252 [==============================] - 0s 257us/step - loss: 0.1314 - acc: 0.9921 - val_loss: 2.4014 - val_acc: 0.2698\n",
      "Epoch 93/100\n",
      "252/252 [==============================] - 0s 210us/step - loss: 0.1284 - acc: 0.9881 - val_loss: 2.4137 - val_acc: 0.2381\n",
      "Epoch 94/100\n",
      "252/252 [==============================] - 0s 230us/step - loss: 0.1249 - acc: 0.9921 - val_loss: 2.4089 - val_acc: 0.2698\n",
      "Epoch 95/100\n",
      "252/252 [==============================] - 0s 228us/step - loss: 0.1229 - acc: 0.9881 - val_loss: 2.4260 - val_acc: 0.2540\n",
      "Epoch 96/100\n",
      "252/252 [==============================] - 0s 210us/step - loss: 0.1186 - acc: 0.9921 - val_loss: 2.4266 - val_acc: 0.2540\n",
      "Epoch 97/100\n",
      "252/252 [==============================] - 0s 206us/step - loss: 0.1162 - acc: 0.9841 - val_loss: 2.4425 - val_acc: 0.2540\n",
      "Epoch 98/100\n",
      "252/252 [==============================] - 0s 216us/step - loss: 0.1137 - acc: 0.9921 - val_loss: 2.4309 - val_acc: 0.2540\n",
      "Epoch 99/100\n",
      "252/252 [==============================] - 0s 194us/step - loss: 0.1105 - acc: 0.9921 - val_loss: 2.4368 - val_acc: 0.2381\n",
      "Epoch 100/100\n",
      "252/252 [==============================] - 0s 233us/step - loss: 0.1076 - acc: 0.9921 - val_loss: 2.4377 - val_acc: 0.2698\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2246397f828>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(yelp.category)\n",
    "encoded_Y = encoder.transform(yelp.category)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "# integer encode the documents\n",
    "vocab_size = 6000\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in yelp.lemma]\n",
    "#print(encoded_docs)\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 130\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)\n",
    "# define the model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_size, 32, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(12, activation='sigmoid'))\n",
    "#model.add(Dropout(0.3))\n",
    "#model.add(Dense(12, activation='sigmoid'))\n",
    "#model.add(Dropout(0.3))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_docs,dummy_y,test_size=0.2)\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=64)\n",
    "# evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3485 5715 3871 ...    0    0    0]\n",
      " [ 362 1220 3682 ...    0    0    0]\n",
      " [1321 4691 1402 ...    0    0    0]\n",
      " ...\n",
      " [1924 2579 3050 ...    0    0    0]\n",
      " [4621  593 4812 ...    0    0    0]\n",
      " [1484 3643 2692 ...    0    0    0]]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 130, 32)           192000    \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 12)                49932     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12)                0         \n",
      "=================================================================\n",
      "Total params: 241,932\n",
      "Trainable params: 241,932\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 252 samples, validate on 63 samples\n",
      "Epoch 1/50\n",
      "252/252 [==============================] - 0s 2ms/step - loss: 6.2438 - acc: 0.1111 - val_loss: 2.4399 - val_acc: 0.2222\n",
      "Epoch 2/50\n",
      "252/252 [==============================] - 0s 224us/step - loss: 6.5951 - acc: 0.1429 - val_loss: 2.4131 - val_acc: 0.2222\n",
      "Epoch 3/50\n",
      "252/252 [==============================] - 0s 202us/step - loss: 5.0563 - acc: 0.1746 - val_loss: 2.4066 - val_acc: 0.0952\n",
      "Epoch 4/50\n",
      "252/252 [==============================] - 0s 245us/step - loss: 6.0163 - acc: 0.1865 - val_loss: 2.4176 - val_acc: 0.1270\n",
      "Epoch 5/50\n",
      "252/252 [==============================] - 0s 271us/step - loss: 6.1627 - acc: 0.1706 - val_loss: 2.4354 - val_acc: 0.1429\n",
      "Epoch 6/50\n",
      "252/252 [==============================] - 0s 188us/step - loss: 6.0469 - acc: 0.1905 - val_loss: 2.4409 - val_acc: 0.1429\n",
      "Epoch 7/50\n",
      "252/252 [==============================] - 0s 245us/step - loss: 5.9891 - acc: 0.1746 - val_loss: 2.4467 - val_acc: 0.1111\n",
      "Epoch 8/50\n",
      "252/252 [==============================] - 0s 206us/step - loss: 6.5072 - acc: 0.1944 - val_loss: 2.4406 - val_acc: 0.1270\n",
      "Epoch 9/50\n",
      "252/252 [==============================] - 0s 216us/step - loss: 6.0952 - acc: 0.1865 - val_loss: 2.4342 - val_acc: 0.1429\n",
      "Epoch 10/50\n",
      "252/252 [==============================] - 0s 210us/step - loss: 6.4959 - acc: 0.1587 - val_loss: 2.4293 - val_acc: 0.1270\n",
      "Epoch 11/50\n",
      "252/252 [==============================] - 0s 186us/step - loss: 6.3264 - acc: 0.1746 - val_loss: 2.4197 - val_acc: 0.1270\n",
      "Epoch 12/50\n",
      "252/252 [==============================] - 0s 190us/step - loss: 6.7613 - acc: 0.1746 - val_loss: 2.4163 - val_acc: 0.1587\n",
      "Epoch 13/50\n",
      "252/252 [==============================] - 0s 241us/step - loss: 6.1091 - acc: 0.1746 - val_loss: 2.4143 - val_acc: 0.1587\n",
      "Epoch 14/50\n",
      "252/252 [==============================] - 0s 206us/step - loss: 6.6259 - acc: 0.2103 - val_loss: 2.4165 - val_acc: 0.1270\n",
      "Epoch 15/50\n",
      "252/252 [==============================] - 0s 238us/step - loss: 6.0817 - acc: 0.2024 - val_loss: 2.4209 - val_acc: 0.1111\n",
      "Epoch 16/50\n",
      "252/252 [==============================] - 0s 210us/step - loss: 6.1147 - acc: 0.1667 - val_loss: 2.4279 - val_acc: 0.1270\n",
      "Epoch 17/50\n",
      "252/252 [==============================] - 0s 206us/step - loss: 6.0628 - acc: 0.1627 - val_loss: 2.4373 - val_acc: 0.1270\n",
      "Epoch 18/50\n",
      "252/252 [==============================] - 0s 232us/step - loss: 6.4740 - acc: 0.1706 - val_loss: 2.4455 - val_acc: 0.1270\n",
      "Epoch 19/50\n",
      "252/252 [==============================] - 0s 257us/step - loss: 6.8023 - acc: 0.2143 - val_loss: 2.4586 - val_acc: 0.1270\n",
      "Epoch 20/50\n",
      "252/252 [==============================] - 0s 214us/step - loss: 6.4882 - acc: 0.2183 - val_loss: 2.4519 - val_acc: 0.1270\n",
      "Epoch 21/50\n",
      "252/252 [==============================] - 0s 182us/step - loss: 6.3401 - acc: 0.2302 - val_loss: 2.4339 - val_acc: 0.1270\n",
      "Epoch 22/50\n",
      "252/252 [==============================] - 0s 210us/step - loss: 6.3952 - acc: 0.2143 - val_loss: 2.4201 - val_acc: 0.1111\n",
      "Epoch 23/50\n",
      "252/252 [==============================] - 0s 185us/step - loss: 6.6400 - acc: 0.2421 - val_loss: 2.4151 - val_acc: 0.2063\n",
      "Epoch 24/50\n",
      "252/252 [==============================] - 0s 214us/step - loss: 6.0393 - acc: 0.2857 - val_loss: 2.4153 - val_acc: 0.1905\n",
      "Epoch 25/50\n",
      "252/252 [==============================] - 0s 178us/step - loss: 6.1942 - acc: 0.2540 - val_loss: 2.4275 - val_acc: 0.1905\n",
      "Epoch 26/50\n",
      "252/252 [==============================] - 0s 230us/step - loss: 5.3302 - acc: 0.2897 - val_loss: 2.4432 - val_acc: 0.1746\n",
      "Epoch 27/50\n",
      "252/252 [==============================] - 0s 220us/step - loss: 6.1106 - acc: 0.2540 - val_loss: 2.4613 - val_acc: 0.1270\n",
      "Epoch 28/50\n",
      "252/252 [==============================] - 0s 214us/step - loss: 6.4675 - acc: 0.2540 - val_loss: 2.4542 - val_acc: 0.1587\n",
      "Epoch 29/50\n",
      "252/252 [==============================] - 0s 223us/step - loss: 6.3265 - acc: 0.3730 - val_loss: 2.4347 - val_acc: 0.1905\n",
      "Epoch 30/50\n",
      "252/252 [==============================] - 0s 210us/step - loss: 6.1436 - acc: 0.4206 - val_loss: 2.4205 - val_acc: 0.1905\n",
      "Epoch 31/50\n",
      "252/252 [==============================] - 0s 176us/step - loss: 5.0135 - acc: 0.4643 - val_loss: 2.3980 - val_acc: 0.2381\n",
      "Epoch 32/50\n",
      "252/252 [==============================] - 0s 191us/step - loss: 5.7904 - acc: 0.4325 - val_loss: 2.4028 - val_acc: 0.1429\n",
      "Epoch 33/50\n",
      "252/252 [==============================] - 0s 210us/step - loss: 5.7880 - acc: 0.4444 - val_loss: 2.4839 - val_acc: 0.1587\n",
      "Epoch 34/50\n",
      "252/252 [==============================] - 0s 198us/step - loss: 5.8375 - acc: 0.4484 - val_loss: 2.5206 - val_acc: 0.1270\n",
      "Epoch 35/50\n",
      "252/252 [==============================] - 0s 214us/step - loss: 6.5143 - acc: 0.4603 - val_loss: 2.4933 - val_acc: 0.1746\n",
      "Epoch 36/50\n",
      "252/252 [==============================] - 0s 214us/step - loss: 6.2102 - acc: 0.5317 - val_loss: 2.4317 - val_acc: 0.1746\n",
      "Epoch 37/50\n",
      "252/252 [==============================] - 0s 186us/step - loss: 5.7663 - acc: 0.5476 - val_loss: 2.4654 - val_acc: 0.1270\n",
      "Epoch 38/50\n",
      "252/252 [==============================] - 0s 184us/step - loss: 5.6737 - acc: 0.5437 - val_loss: 2.4556 - val_acc: 0.2063\n",
      "Epoch 39/50\n",
      "252/252 [==============================] - 0s 190us/step - loss: 5.9836 - acc: 0.5079 - val_loss: 2.4446 - val_acc: 0.1905\n",
      "Epoch 40/50\n",
      "252/252 [==============================] - 0s 232us/step - loss: 5.5913 - acc: 0.5952 - val_loss: 2.4472 - val_acc: 0.1429\n",
      "Epoch 41/50\n",
      "252/252 [==============================] - 0s 224us/step - loss: 4.9239 - acc: 0.6230 - val_loss: 2.4399 - val_acc: 0.2381\n",
      "Epoch 42/50\n",
      "252/252 [==============================] - 0s 186us/step - loss: 6.0383 - acc: 0.5357 - val_loss: 2.5012 - val_acc: 0.2222\n",
      "Epoch 43/50\n",
      "252/252 [==============================] - 0s 218us/step - loss: 5.2909 - acc: 0.5992 - val_loss: 2.5236 - val_acc: 0.1587\n",
      "Epoch 44/50\n",
      "252/252 [==============================] - 0s 224us/step - loss: 5.3318 - acc: 0.6151 - val_loss: 2.4849 - val_acc: 0.1429\n",
      "Epoch 45/50\n",
      "252/252 [==============================] - 0s 216us/step - loss: 5.0286 - acc: 0.5952 - val_loss: 2.4082 - val_acc: 0.1587\n",
      "Epoch 46/50\n",
      "252/252 [==============================] - 0s 198us/step - loss: 5.9256 - acc: 0.6032 - val_loss: 2.4421 - val_acc: 0.2381\n",
      "Epoch 47/50\n",
      "252/252 [==============================] - 0s 202us/step - loss: 4.8147 - acc: 0.6627 - val_loss: 2.4914 - val_acc: 0.1429\n",
      "Epoch 48/50\n",
      "252/252 [==============================] - 0s 184us/step - loss: 5.9441 - acc: 0.6032 - val_loss: 2.5185 - val_acc: 0.0952\n",
      "Epoch 49/50\n",
      "252/252 [==============================] - 0s 187us/step - loss: 5.3526 - acc: 0.6310 - val_loss: 2.4560 - val_acc: 0.1905\n",
      "Epoch 50/50\n",
      "252/252 [==============================] - 0s 190us/step - loss: 5.4786 - acc: 0.6151 - val_loss: 2.4381 - val_acc: 0.1746\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22462914390>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(yelp.category)\n",
    "encoded_Y = encoder.transform(yelp.category)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "from keras.layers import Dropout\n",
    "# integer encode the documents\n",
    "vocab_size = 6000\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in yelp.lemma]\n",
    "#print(encoded_docs)\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 130\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)\n",
    "# define the model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_size, 32, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(12, activation='sigmoid'))\n",
    "model.add(Dropout(0.3))\n",
    "#model.add(Dense(12, activation='sigmoid'))\n",
    "#model.add(Dropout(0.3))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_docs,dummy_y,test_size=0.2)\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=64)\n",
    "# evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# embedding_vecor_length = 32\n",
    "# model2 = Sequential()\n",
    "# model2.add(Embedding(vocab_size, embedding_vecor_length,input_length=max_length))\n",
    "# from keras.layers import LSTM\n",
    "# model2.add(LSTM(100))\n",
    "# model2.add(Dense(12, activation='sigmoid'))\n",
    "# model2.add(Dense(12, activation='sigmoid'))\n",
    "# model2.add(Dense(12, activation='sigmoid'))\n",
    "# model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# print(model.summary())\n",
    "# model2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from keras.layers import Dropout\n",
    "# embedding_vecor_length = 32\n",
    "# model3 = Sequential()\n",
    "# model3.add(Embedding(vocab_size, embedding_vecor_length,input_length=max_length))\n",
    "\n",
    "# model3.add(Dense(12, activation='sigmoid')) \n",
    "# model3.add(Dropout(0.3))\n",
    "# model3.add(Dense(12, activation='sigmoid'))\n",
    "# model3.add(Dropout(0.3))\n",
    "# model3.add(Dense(12, activation='sigmoid'))\n",
    "# model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# print(model.summary())\n",
    "# model3.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 32, input_length=max_length))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(12, activation='sigmoid'))\n",
    "    \n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# model = KerasClassifier(build_fn=create_model,verbose=0)\n",
    "# # define the grid search parameters\n",
    "# # learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "# # momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "# epochs=[1,3,5]\n",
    "# batch_size=[32,64,128,256]\n",
    "# param_grid = dict(epochs=epochs,batch_size=batch_size)\n",
    "# grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "\n",
    "# grid_result = grid.fit(X_train, y_train)\n",
    "# # summarize results\n",
    "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "# means = grid_result.cv_results_['mean_test_score']\n",
    "# stds = grid_result.cv_results_['std_test_score']\n",
    "# params = grid_result.cv_results_['params']\n",
    "# for mean, stdev, param in zip(means, stds, params):\n",
    "#     print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
