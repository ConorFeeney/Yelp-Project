{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Review\n",
    "## Business Problem\n",
    "In today’s information era, even non technological\n",
    "savvy consumers can quickly discover information\n",
    "about a business. This is great for customers, as it\n",
    "allows them to identify the “snake oil salesmen” (this\n",
    "is a metaphor that was coined for individuals who sold\n",
    "fraudulent goods). The motivation behind this project\n",
    "is to develop a tool that can show businesses, in this\n",
    "case restaurants, what aspects of their business are\n",
    "leading to their reviews without them having to crawl\n",
    "through potentially hundreds of text data. Is it their\n",
    "price? Their food? Their service? Businesses that know\n",
    "this information can get ahead of the competition by\n",
    "improving the areas that lead to less positive reviews.\n",
    "For example, a restaurant that knows they possess four\n",
    "stars on Yelp, along with a breakdown of which\n",
    "aspects of their establishment contributed to these four\n",
    "stars might see that they lose marks on their service.\n",
    "This may encourage management to retrain\n",
    "waiters/waitresses in an effort to improve their service\n",
    "\n",
    "score and thus their overall Yelp score. This prospect\n",
    "would be extremely useful to restaurants because when\n",
    "was the last time the reader went to a restaurant\n",
    "without reading some online reviews beforehand?\n",
    "\n",
    "## The Data\n",
    "The data, as discussed above, is text review data. In this re-enforcement learning stage, the data set comes from two locations. One set is a snapshot of about 10,000 records from a larger database where the trained model will be deployed on. This snapshot was manually coded with one of the following categories\n",
    "* Food#Postive\n",
    "* Food#Negative\n",
    "* Price#Postive\n",
    "* Price#Negative\n",
    "* Quality#Positive\n",
    "* Quality#Negative\n",
    "* Restaurant#Positive\n",
    "* Restaurant#Negative\n",
    "* Location#Postive\n",
    "* Location#Negative\n",
    "* Service#Positive\n",
    "* Service#Negative\n",
    "This was done for validation purposes. The other 1,600 records come from a Yelp Kaggle competition. The categories were coded to reflect categories above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1 Read in data\n",
    "# Step 2 Preprocess text data\n",
    "# Step 3 Word Embedding\n",
    "# Step 4 Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "import re\n",
    "# importing keras packages\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "from keras.layers import Flatten\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential #### required layer in our LSTM network\n",
    "from keras.layers import Dense #### required layer in our LSTM network\n",
    " #### required layer in our LSTM network\n",
    "from keras.layers.embeddings import Embedding #### required layer in our LSTM network\n",
    "from keras.preprocessing import sequence #### Packaged preprocessing step in Keras\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yelp=pd.read_csv('all_data20180608.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of        Unnamed: 0             category  \\\n",
       "0               0     SERVICE#POSITIVE   \n",
       "1               1     SERVICE#POSITIVE   \n",
       "2               2     SERVICE#POSITIVE   \n",
       "3               3     SERVICE#POSITIVE   \n",
       "4               4     SERVICE#POSITIVE   \n",
       "5               5     SERVICE#POSITIVE   \n",
       "6               6     SERVICE#POSITIVE   \n",
       "7               7     SERVICE#POSITIVE   \n",
       "8               8     SERVICE#POSITIVE   \n",
       "9               9     SERVICE#POSITIVE   \n",
       "10             10     SERVICE#POSITIVE   \n",
       "11             11     SERVICE#POSITIVE   \n",
       "12             12     SERVICE#POSITIVE   \n",
       "13             13     SERVICE#POSITIVE   \n",
       "14             14     SERVICE#POSITIVE   \n",
       "15             15     SERVICE#POSITIVE   \n",
       "16             16     SERVICE#POSITIVE   \n",
       "17             17     SERVICE#POSITIVE   \n",
       "18             18     SERVICE#POSITIVE   \n",
       "19             19     SERVICE#POSITIVE   \n",
       "20             20     SERVICE#POSITIVE   \n",
       "21             21     SERVICE#POSITIVE   \n",
       "22             22     SERVICE#POSITIVE   \n",
       "23             23     SERVICE#POSITIVE   \n",
       "24             24     SERVICE#POSITIVE   \n",
       "25             25     SERVICE#POSITIVE   \n",
       "26             26     SERVICE#POSITIVE   \n",
       "27             27     SERVICE#POSITIVE   \n",
       "28             28     SERVICE#POSITIVE   \n",
       "29             29     SERVICE#POSITIVE   \n",
       "...           ...                  ...   \n",
       "12522        1624     SERVICE#POSITIVE   \n",
       "12523        1625    LOCATION#POSITIVE   \n",
       "12524        1626        FOOD#POSITIVE   \n",
       "12525        1627  RESTAURANT#POSITIVE   \n",
       "12526        1628  RESTAURANT#POSITIVE   \n",
       "12527        1629  RESTAURANT#POSITIVE   \n",
       "12528        1630     SERVICE#POSITIVE   \n",
       "12529        1631    LOCATION#POSITIVE   \n",
       "12530        1632  RESTAURANT#POSITIVE   \n",
       "12531        1633  RESTAURANT#POSITIVE   \n",
       "12532        1634        FOOD#POSITIVE   \n",
       "12533        1635    LOCATION#POSITIVE   \n",
       "12534        1636     QUALITY#POSITIVE   \n",
       "12535        1637     QUALITY#POSITIVE   \n",
       "12536        1638  RESTAURANT#POSITIVE   \n",
       "12537        1639     SERVICE#POSITIVE   \n",
       "12538        1640     SERVICE#POSITIVE   \n",
       "12539        1641     SERVICE#POSITIVE   \n",
       "12540        1642     SERVICE#POSITIVE   \n",
       "12541        1643    LOCATION#POSITIVE   \n",
       "12542        1644    LOCATION#POSITIVE   \n",
       "12543        1645     QUALITY#POSITIVE   \n",
       "12544        1646     QUALITY#POSITIVE   \n",
       "12545        1647  RESTAURANT#POSITIVE   \n",
       "12546        1648  RESTAURANT#POSITIVE   \n",
       "12547        1649     QUALITY#POSITIVE   \n",
       "12548        1650     QUALITY#POSITIVE   \n",
       "12549        1651     QUALITY#POSITIVE   \n",
       "12550        1652     QUALITY#POSITIVE   \n",
       "12551        1653     QUALITY#POSITIVE   \n",
       "\n",
       "                                                    text  \n",
       "0      My Friend Gabi,\\r\\n\\r\\nI love your cute Parisi...  \n",
       "1       Had a good waiter, all the staff were very cool.  \n",
       "2      My only regret is not catching the name of our...  \n",
       "3      Lotus of Siam did not disappoint, the service ...  \n",
       "4      His name is Carlos if you ever want to request...  \n",
       "5      The room is so beautiful and the servers are g...  \n",
       "6      Service was quick, the price was A-OK, and you...  \n",
       "7                            Good service and good food.  \n",
       "8      Say what you will about the location and decor...  \n",
       "9            Service was snappy and the food very tasty.  \n",
       "10     Came here a few months ago and while the food ...  \n",
       "11     The hostess and waitress were both very friend...  \n",
       "12     Shout out to my boy Wesley...the host...he was...  \n",
       "13        Waitress was awesome, helpful and on the ball.  \n",
       "14     ), the service was great and it was a busy aft...  \n",
       "15     We arrived just before 3pm on a weekday, and w...  \n",
       "16     They were happy to help take as many pictures ...  \n",
       "17     Some of the items I've ordered in Mon Ami Gabi...  \n",
       "18                          Wonderful attentive service.  \n",
       "19     Dustin Mohawk was friendly and did a great job...  \n",
       "20           Food, service, almost everything was great!  \n",
       "21     Having said that the service there was top notch.  \n",
       "22     The ambiance, the service, the accommodating a...  \n",
       "23     I had a great day, the hostess was friendly, t...  \n",
       "24     Sampling only the breakfast options, Gabi aime...  \n",
       "25     The wine list is good, and it helps when they ...  \n",
       "26     Our waters were never empty, the manager stopp...  \n",
       "27            The food and service were both impeccable.  \n",
       "28          Good food, beautiful setting, great service.  \n",
       "29     Sat at the bar...bartenders were cute and atte...  \n",
       "...                                                  ...  \n",
       "12522  Interesting selection, good wines, service fin...  \n",
       "12523  Interesting selection, good wines, service fin...  \n",
       "12524  Interesting selection, good wines, service fin...  \n",
       "12525                                         I love it.  \n",
       "12526           I plan on stopping by next week as well.  \n",
       "12527  I found it on a cold night, the perfect spot t...  \n",
       "12528            I recieved prompt service with a smile.  \n",
       "12529  To me it exemplifies Soho, cute, artsy, intere...  \n",
       "12530                         Definately check it out!!!  \n",
       "12531  This place blew me away...by far my new favori...  \n",
       "12532         The wine list is extensive and impressive.  \n",
       "12533    LOVE the atmosphere - felt like I was in Paris.  \n",
       "12534  The mussels were fantastic and so was the dess...  \n",
       "12535  The mussels were fantastic and so was the dess...  \n",
       "12536  The mussels were fantastic and so was the dess...  \n",
       "12537  I have been coming here for years and have not...  \n",
       "12538  I have been coming here for years and have not...  \n",
       "12539  Over the years the host, Vittorio, and his cre...  \n",
       "12540  Over the years the host, Vittorio, and his cre...  \n",
       "12541  I've also been amazed at all the new additions...  \n",
       "12542  I've also been amazed at all the new additions...  \n",
       "12543  I've also been amazed at all the new additions...  \n",
       "12544  I've also been amazed at all the new additions...  \n",
       "12545                                 I lOVE THIS PLACE!  \n",
       "12546                        Keep up the good work guys!  \n",
       "12547  I have to say I have never had a disapointing ...  \n",
       "12548  We could have made a meal of the yummy dumplin...  \n",
       "12549  Luckily we saved room for the BBQ Salmon, Sea ...  \n",
       "12550  Luckily we saved room for the BBQ Salmon, Sea ...  \n",
       "12551  Luckily we saved room for the BBQ Salmon, Sea ...  \n",
       "\n",
       "[12552 rows x 3 columns]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analytics\n",
    "Text analytics means examining text that was written by, or about, customers. You find patterns and topics of interest, and then take practical action based on what you learn.\n",
    "\n",
    "Text analytics can be performed manually, but it is an inefficient process. Therefore, text analytics software has been created that uses text mining and natural language processing algorithms to find meaning in huge amounts of text, which is the attempt of this project.\n",
    "\n",
    "### PreProcessing\n",
    "Unlike regular data preprocessing steps, text mining requires an alternate approach. The end goal is more or less the same, a normalised data set that is numbers only containing less noise. \n",
    "\n",
    "#### Lower Case \n",
    "This is a step that helps put the text on an equal footing. The most simple way to explain it is as follows: One wouldn't count *Everything* and *everything* as two separate words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PreProcessing\n",
    "#step 1 lower case\n",
    "#step 2 punctuation\n",
    "#step 3 stop word\n",
    "#step 4 common word removal\n",
    "#step 5 rare word removal\n",
    "#step 6 token\n",
    "#step 7 stemming\n",
    "#step 8 lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    my friend gabi, i love your cute parisian inte...\n",
       "1     had a good waiter, all the staff were very cool.\n",
       "2    my only regret is not catching the name of our...\n",
       "3    lotus of siam did not disappoint, the service ...\n",
       "4    his name is carlos if you ever want to request...\n",
       "Name: lower, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step 1\n",
    "yelp['lower'] = yelp.text.apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "yelp.lower.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Punctuation\n",
    "Again, this is just to keep things equal and prevent items like full stops from adding noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    my friend gabi i love your cute parisian inter...\n",
       "1       had a good waiter all the staff were very cool\n",
       "2    my only regret is not catching the name of our...\n",
       "3    lotus of siam did not disappoint the service w...\n",
       "4    his name is carlos if you ever want to request...\n",
       "Name: no_punc, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step 2\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "reg_tok = RegexpTokenizer(r'\\w+')#+ is one or more\n",
    "yelp['no_punc'] = yelp['lower'].apply(lambda x: ' '.join(reg_tok.tokenize(x)))\n",
    "yelp.no_punc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop Words\n",
    "Stop words are those words which are filtered out before further processing of text, since these words contribute little to overall meaning, given that they are generally the most common words in a language. For instance, \"the,\" \"and,\" and \"a,\" while all required words in a particular passage, don't generally contribute greatly to one's understanding of content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    friend gabi love cute parisian interior dim li...\n",
       "1                               good waiter staff cool\n",
       "2    regret catching name server best experienced f...\n",
       "3        lotus siam disappoint service great attentive\n",
       "4          name carlos ever want request service great\n",
       "Name: no_stop, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step 3\n",
    "yelp['no_stop'] = yelp['no_punc'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "yelp.no_stop.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common and Rare Words\n",
    "Removing the most common and the rarest words often can add a small amount of value to your model. This is because words like \"good\" or \"food\", often don't add a specific value to a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "food          3504\n",
       "good          1925\n",
       "buffet        1556\n",
       "service       1554\n",
       "great         1365\n",
       "place         1111\n",
       "vegas          882\n",
       "like           764\n",
       "restaurant     656\n",
       "one            642\n",
       "get            641\n",
       "best           635\n",
       "really         625\n",
       "quality        611\n",
       "price          596\n",
       "would          552\n",
       "time           539\n",
       "go             539\n",
       "selection      470\n",
       "better         463\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = pd.Series(' '.join(yelp['no_stop']).split()).value_counts()[:20]#combining all rows and then splitting and converitign and value count\n",
    "freq\n",
    "#looking at these, we actually want to keep them so no need to carry out this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#step 4\n",
    "rare = pd.Series(' '.join(yelp['no_stop']).split()).value_counts()[-600:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#step 5\n",
    "rare = list(rare.index)\n",
    "yelp['no_rare'] = yelp['no_stop'].apply(lambda x: \" \".join(x for x in x.split() if x not in rare))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    friend gave love cut parisian interior dim lig...\n",
       "1                               good waiter staff cool\n",
       "2    regret catching name server best experienced f...\n",
       "3         lots siam disappoint service great attentive\n",
       "4           name carlo ever want request service great\n",
       "Name: no_stop, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just did for note\n",
    "from textblob import TextBlob\n",
    "# not really doing that for tutorial, this is just demo of it\n",
    "yelp['no_stop'][:5].apply(lambda x: str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenising\n",
    "Tokenization is a step which splits longer strings of text into smaller pieces, or tokens. Larger chunks of text can be tokenized into sentences, sentences can be tokenized into words, etc. Further processing is generally performed after a piece of text has been appropriately tokenized. Tokenization is also referred to as text segmentation or lexical analysis. Sometimes segmentation is used to refer to the breakdown of a large chunk of text into pieces larger than words (e.g. paragraphs or sentences), while tokenization is reserved for the breakdown process which results exclusively in words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    friend gabi love cute parisian interior dim li...\n",
       "1                               good waiter staff cool\n",
       "2    regret catching name server best experienced f...\n",
       "3        lotus siam disappoint service great attentive\n",
       "4          name carlos ever want request service great\n",
       "Name: token, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step 6\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "_word_tokenize = TreebankWordTokenizer()\n",
    "yelp['token'] = yelp['no_rare'].apply(lambda x: ' '.join(_word_tokenize.tokenize(x)))\n",
    "yelp.token.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "Stemming is the process of eliminating affixes (suffixed, prefixes, infixes, circumfixes) from a word in order to obtain a word stem. For example, running -> run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    friend gabi love cute parisian interior dim li...\n",
       "1                               good waiter staff cool\n",
       "2    regret catch name server best experienc far tr...\n",
       "3            lotus siam disappoint servic great attent\n",
       "4            name carlo ever want request servic great\n",
       "Name: stemed, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step 7\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "st = SnowballStemmer(\"english\")\n",
    "yelp['stemed']=yelp['token'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "yelp.stemed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Lemmatization is related to stemming, differing in that lemmatization is able to capture canonical forms based on a word's lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     friend gabi love cute parisian interior dim li...\n",
       "1                                good waiter staff cool\n",
       "2     regret catch name server best experienc far tr...\n",
       "3             lotus siam disappoint servic great attent\n",
       "4             name carlo ever want request servic great\n",
       "5                               room beauti server good\n",
       "6     servic quick price ok get pretti darn good san...\n",
       "7                                 good servic good food\n",
       "8     say locat decor lotus siam never life find bet...\n",
       "9                              servic snappi food tasti\n",
       "10    came month ago food ok initi encount cashier g...\n",
       "11                       hostess waitress friend attent\n",
       "12                     shout boy wesley host cool peopl\n",
       "13                            waitress awesom help ball\n",
       "14     servic great busi afternoon outdoor set look day\n",
       "15    arriv 3pm weekday prompt seat busi patio time ...\n",
       "16    happi help take mani pictur request alway kept...\n",
       "17    item order mon ami gabi oyster du jour 15 95 w...\n",
       "18                                 wonder attent servic\n",
       "19    dustin mohawk friend great job enhanc impress ...\n",
       "Name: lemma, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step 8\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "yelp['lemma']=yelp['stemed'].apply(lambda x: \" \".join([wordnet_lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "yelp.lemma.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding \n",
    "Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with one dimension per word to a continuous vector space with a much lower dimension.\n",
    "\n",
    "Methods to generate this mapping include neural networks,[1] dimensionality reduction on the word co-occurrence matrix, probabilistic models,explainable knowledge base method,and explicit representation in terms of the context in which words appear. They are needed as computers do not understand words, and in laymans terms convert words to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "596"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepping the Word Embedding by getting dictionary length and max sentence length\n",
    "\n",
    "yelp.lemma.str.len().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "count=Counter(\" \".join(yelp.lemma).split(\" \")).items()\n",
    "# print(sorted(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5246"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#length of dictionary\n",
    "len(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like singl littl dish put tast portion deep fri broccoli chees casserol surpris favorit american plate love littl bucket tater tot waffl fri mini fri basket piec fri chicken sweet potato fri brisket nice outsid like option bbq sauc red velvet whoopi pie soft point flavor authent dessert varieti cupcak cooki bread pud uniqu gelato flavor made order crepe sugar free dessert ton choos midlight good amount empti spot item look good guess popular ran lowlight shrimp cold one tast bit fishi hot one head overlook spici fri fish excit dish great probabl sit meat dri item great buffet other mediocr\n"
     ]
    }
   ],
   "source": [
    "#longest sentence\n",
    "print(max(yelp.lemma, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# embeddings = tf.Variable(\n",
    "#     tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "# embed = tf.nn.embedding_lookup(embeddings, train_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 406 3963 5829 ...    0    0    0]\n",
      " [1579 5282   27 ...    0    0    0]\n",
      " [4362 4502 2098 ...    0    0    0]\n",
      " ...\n",
      " [4270 2852  513 ...    0    0    0]\n",
      " [4270 2852  513 ...    0    0    0]\n",
      " [4270 2852  513 ...    0    0    0]]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 130, 32)           192000    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                49932     \n",
      "=================================================================\n",
      "Total params: 241,932\n",
      "Trainable params: 241,932\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 10041 samples, validate on 2511 samples\n",
      "Epoch 1/30\n",
      "10041/10041 [==============================] - 1s 137us/step - loss: 2.3376 - acc: 0.1850 - val_loss: 2.3070 - val_acc: 0.1908\n",
      "Epoch 2/30\n",
      "10041/10041 [==============================] - 1s 121us/step - loss: 2.0758 - acc: 0.2774 - val_loss: 1.8680 - val_acc: 0.3640\n",
      "Epoch 3/30\n",
      "10041/10041 [==============================] - 1s 111us/step - loss: 1.6600 - acc: 0.4310 - val_loss: 1.6878 - val_acc: 0.4182\n",
      "Epoch 4/30\n",
      "10041/10041 [==============================] - 1s 117us/step - loss: 1.4701 - acc: 0.4959 - val_loss: 1.6367 - val_acc: 0.4225\n",
      "Epoch 5/30\n",
      "10041/10041 [==============================] - 1s 117us/step - loss: 1.3453 - acc: 0.5409 - val_loss: 1.6323 - val_acc: 0.4353\n",
      "Epoch 6/30\n",
      "10041/10041 [==============================] - 1s 117us/step - loss: 1.2520 - acc: 0.5727 - val_loss: 1.6251 - val_acc: 0.4432\n",
      "Epoch 7/30\n",
      "10041/10041 [==============================] - 1s 116us/step - loss: 1.1766 - acc: 0.5959 - val_loss: 1.6436 - val_acc: 0.4432\n",
      "Epoch 8/30\n",
      "10041/10041 [==============================] - 1s 117us/step - loss: 1.1049 - acc: 0.6231 - val_loss: 1.6656 - val_acc: 0.4365\n",
      "Epoch 9/30\n",
      "10041/10041 [==============================] - 1s 118us/step - loss: 1.0438 - acc: 0.6397 - val_loss: 1.6907 - val_acc: 0.4357\n",
      "Epoch 10/30\n",
      "10041/10041 [==============================] - 1s 129us/step - loss: 0.9866 - acc: 0.6631 - val_loss: 1.7300 - val_acc: 0.4245\n",
      "Epoch 11/30\n",
      "10041/10041 [==============================] - 1s 136us/step - loss: 0.9363 - acc: 0.6755 - val_loss: 1.7624 - val_acc: 0.4186\n",
      "Epoch 12/30\n",
      "10041/10041 [==============================] - 1s 136us/step - loss: 0.8885 - acc: 0.6850 - val_loss: 1.7947 - val_acc: 0.4170\n",
      "Epoch 13/30\n",
      "10041/10041 [==============================] - 1s 135us/step - loss: 0.8437 - acc: 0.6994 - val_loss: 1.8491 - val_acc: 0.4054\n",
      "Epoch 14/30\n",
      "10041/10041 [==============================] - 1s 136us/step - loss: 0.8067 - acc: 0.7080 - val_loss: 1.8912 - val_acc: 0.3994\n",
      "Epoch 15/30\n",
      "10041/10041 [==============================] - 1s 134us/step - loss: 0.7694 - acc: 0.7181 - val_loss: 1.9063 - val_acc: 0.4046\n",
      "Epoch 16/30\n",
      "10041/10041 [==============================] - 1s 118us/step - loss: 0.7335 - acc: 0.7246 - val_loss: 1.9518 - val_acc: 0.3963\n",
      "Epoch 17/30\n",
      "10041/10041 [==============================] - 1s 120us/step - loss: 0.7040 - acc: 0.7338 - val_loss: 1.9956 - val_acc: 0.3939\n",
      "Epoch 18/30\n",
      "10041/10041 [==============================] - 1s 119us/step - loss: 0.6770 - acc: 0.7394 - val_loss: 2.0456 - val_acc: 0.3883\n",
      "Epoch 19/30\n",
      "10041/10041 [==============================] - 1s 122us/step - loss: 0.6534 - acc: 0.7425 - val_loss: 2.0801 - val_acc: 0.3907\n",
      "Epoch 20/30\n",
      "10041/10041 [==============================] - 1s 120us/step - loss: 0.6299 - acc: 0.7459 - val_loss: 2.1180 - val_acc: 0.3823\n",
      "Epoch 21/30\n",
      "10041/10041 [==============================] - 1s 119us/step - loss: 0.6088 - acc: 0.7487 - val_loss: 2.1783 - val_acc: 0.3763\n",
      "Epoch 22/30\n",
      "10041/10041 [==============================] - 1s 121us/step - loss: 0.5893 - acc: 0.7506 - val_loss: 2.1946 - val_acc: 0.3779\n",
      "Epoch 23/30\n",
      "10041/10041 [==============================] - 1s 143us/step - loss: 0.5722 - acc: 0.7551 - val_loss: 2.2316 - val_acc: 0.3767\n",
      "Epoch 24/30\n",
      "10041/10041 [==============================] - 1s 136us/step - loss: 0.5546 - acc: 0.7609 - val_loss: 2.2703 - val_acc: 0.3728\n",
      "Epoch 25/30\n",
      "10041/10041 [==============================] - 1s 134us/step - loss: 0.5396 - acc: 0.7625 - val_loss: 2.3073 - val_acc: 0.3740\n",
      "Epoch 26/30\n",
      "10041/10041 [==============================] - 1s 134us/step - loss: 0.5264 - acc: 0.7642 - val_loss: 2.3615 - val_acc: 0.3696\n",
      "Epoch 27/30\n",
      "10041/10041 [==============================] - 1s 133us/step - loss: 0.5144 - acc: 0.7683 - val_loss: 2.3853 - val_acc: 0.3632\n",
      "Epoch 28/30\n",
      "10041/10041 [==============================] - 1s 128us/step - loss: 0.5049 - acc: 0.7698 - val_loss: 2.4225 - val_acc: 0.3648\n",
      "Epoch 29/30\n",
      "10041/10041 [==============================] - 1s 120us/step - loss: 0.4931 - acc: 0.7714 - val_loss: 2.4567 - val_acc: 0.3680\n",
      "Epoch 30/30\n",
      "10041/10041 [==============================] - 1s 118us/step - loss: 0.4823 - acc: 0.7721 - val_loss: 2.4799 - val_acc: 0.3624\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1030a6d8>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# define documents\n",
    "\n",
    "# define class labels\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(yelp.category)\n",
    "encoded_Y = encoder.transform(yelp.category)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "# integer encode the documents\n",
    "vocab_size = 6000\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in yelp.lemma]\n",
    "#print(encoded_docs)\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 130\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)\n",
    "# define the model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_size, 32, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(12, activation='sigmoid'))\n",
    "#model.add(Dropout(0.3))\n",
    "#model.add(Dense(12, activation='sigmoid'))\n",
    "#model.add(Dropout(0.3))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_docs,dummy_y,test_size=0.2)\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=30, batch_size=64)\n",
    "# evaluate the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# embedding_vecor_length = 32\n",
    "# model2 = Sequential()\n",
    "# model2.add(Embedding(vocab_size, embedding_vecor_length,input_length=max_length))\n",
    "# from keras.layers import LSTM\n",
    "# model2.add(LSTM(100))\n",
    "# model2.add(Dense(12, activation='sigmoid'))\n",
    "# model2.add(Dense(12, activation='sigmoid'))\n",
    "# model2.add(Dense(12, activation='sigmoid'))\n",
    "# model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# print(model.summary())\n",
    "# model2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from keras.layers import Dropout\n",
    "# embedding_vecor_length = 32\n",
    "# model3 = Sequential()\n",
    "# model3.add(Embedding(vocab_size, embedding_vecor_length,input_length=max_length))\n",
    "\n",
    "# model3.add(Dense(12, activation='sigmoid')) \n",
    "# model3.add(Dropout(0.3))\n",
    "# model3.add(Dense(12, activation='sigmoid'))\n",
    "# model3.add(Dropout(0.3))\n",
    "# model3.add(Dense(12, activation='sigmoid'))\n",
    "# model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# print(model.summary())\n",
    "# model3.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 32, input_length=max_length))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(12, activation='sigmoid'))\n",
    "    \n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model,verbose=0)\n",
    "# define the grid search parameters\n",
    "# learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "# momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "epochs=[1,3,5]\n",
    "batch_size=[32,64,128,256]\n",
    "param_grid = dict(epochs=epochs,batch_size=batch_size)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
