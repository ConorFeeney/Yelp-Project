{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Review\n",
    "## Business Problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1 Read in data\n",
    "# Step 2 Preprocess text data\n",
    "# Step 3 Word Embedding\n",
    "# Step 4 Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "import re\n",
    "# importing keras packages\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "from keras.layers import Flatten\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential #### required layer in our LSTM network\n",
    "from keras.layers import Dense #### required layer in our LSTM network\n",
    " #### required layer in our LSTM network\n",
    "from keras.layers.embeddings import Embedding #### required layer in our LSTM network\n",
    "from keras.preprocessing import sequence #### Packaged preprocessing step in Keras\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yelp=pd.read_csv('all_data20180608.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PreProcessing\n",
    "#step 1 lower case\n",
    "#step 2 punctuation\n",
    "#step 3 stop word\n",
    "#step 4 common word removal\n",
    "#step 5 rare word removal\n",
    "#step 6 token\n",
    "#step 7 stemming\n",
    "#step 8 lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    my friend gabi, i love your cute parisian inte...\n",
       "1     had a good waiter, all the staff were very cool.\n",
       "2    my only regret is not catching the name of our...\n",
       "3    lotus of siam did not disappoint, the service ...\n",
       "4    his name is carlos if you ever want to request...\n",
       "Name: lower, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step 1\n",
    "yelp['lower'] = yelp.text.apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "yelp.lower.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    my friend gabi i love your cute parisian inter...\n",
       "1       had a good waiter all the staff were very cool\n",
       "2    my only regret is not catching the name of our...\n",
       "3    lotus of siam did not disappoint the service w...\n",
       "4    his name is carlos if you ever want to request...\n",
       "Name: no_punc, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step 2\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "reg_tok = RegexpTokenizer(r'\\w+')#+ is one or more\n",
    "yelp['no_punc'] = yelp['lower'].apply(lambda x: ' '.join(reg_tok.tokenize(x)))\n",
    "yelp.no_punc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    friend gabi love cute parisian interior dim li...\n",
       "1                               good waiter staff cool\n",
       "2    regret catching name server best experienced f...\n",
       "3        lotus siam disappoint service great attentive\n",
       "4          name carlos ever want request service great\n",
       "Name: no_stop, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step 3\n",
    "yelp['no_stop'] = yelp['no_punc'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "yelp.no_stop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "food          3504\n",
       "good          1925\n",
       "buffet        1556\n",
       "service       1554\n",
       "great         1365\n",
       "place         1111\n",
       "vegas          882\n",
       "like           764\n",
       "restaurant     656\n",
       "one            642\n",
       "get            641\n",
       "best           635\n",
       "really         625\n",
       "quality        611\n",
       "price          596\n",
       "would          552\n",
       "go             539\n",
       "time           539\n",
       "selection      470\n",
       "better         463\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = pd.Series(' '.join(yelp['no_stop']).split()).value_counts()[:20]#combining all rows and then splitting and converitign and value count\n",
    "freq\n",
    "#looking at these, we actually want to keep them so no need to carry out this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 4\n",
    "rare = pd.Series(' '.join(yelp['no_stop']).split()).value_counts()[-600:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#step 5\n",
    "rare = list(rare.index)\n",
    "yelp['no_rare'] = yelp['no_stop'].apply(lambda x: \" \".join(x for x in x.split() if x not in rare))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    friend gave love cut parisian interior dim lig...\n",
       "1                               good waiter staff cool\n",
       "2    regret catching name server best experienced f...\n",
       "3         lots siam disappoint service great attentive\n",
       "4           name carlo ever want request service great\n",
       "Name: no_stop, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just did for note\n",
    "from textblob import TextBlob\n",
    "# not really doing that for tutorial, this is just demo of it\n",
    "yelp['no_stop'][:5].apply(lambda x: str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    friend gabi love cute parisian interior dim li...\n",
       "1                               good waiter staff cool\n",
       "2    regret catching name server best experienced f...\n",
       "3        lotus siam disappoint service great attentive\n",
       "4          name carlos ever want request service great\n",
       "Name: token, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step 6\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "_word_tokenize = TreebankWordTokenizer()\n",
    "yelp['token'] = yelp['no_rare'].apply(lambda x: ' '.join(_word_tokenize.tokenize(x)))\n",
    "yelp.token.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    friend gabi love cute parisian interior dim li...\n",
       "1                               good waiter staff cool\n",
       "2    regret catch name server best experienc far tr...\n",
       "3            lotus siam disappoint servic great attent\n",
       "4            name carlo ever want request servic great\n",
       "Name: stemed, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step 7\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "st = SnowballStemmer(\"english\")\n",
    "yelp['stemed']=yelp['token'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "yelp.stemed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     friend gabi love cute parisian interior dim li...\n",
       "1                                good waiter staff cool\n",
       "2     regret catch name server best experienc far tr...\n",
       "3             lotus siam disappoint servic great attent\n",
       "4             name carlo ever want request servic great\n",
       "5                               room beauti server good\n",
       "6     servic quick price ok get pretti darn good san...\n",
       "7                                 good servic good food\n",
       "8     say locat decor lotus siam never life find bet...\n",
       "9                              servic snappi food tasti\n",
       "10    came month ago food ok initi encount cashier g...\n",
       "11                       hostess waitress friend attent\n",
       "12                     shout boy wesley host cool peopl\n",
       "13                            waitress awesom help ball\n",
       "14     servic great busi afternoon outdoor set look day\n",
       "15    arriv 3pm weekday prompt seat busi patio time ...\n",
       "16    happi help take mani pictur request alway kept...\n",
       "17    item order mon ami gabi oyster du jour 15 95 w...\n",
       "18                                 wonder attent servic\n",
       "19    dustin mohawk friend great job enhanc impress ...\n",
       "Name: lemma, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step 8\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "yelp['lemma']=yelp['stemed'].apply(lambda x: \" \".join([wordnet_lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "yelp.lemma.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "607"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepping the Word Embedding by getting dictionary length and max sentence length\n",
    "\n",
    "yelp.lemma.str.len().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "count=Counter(\" \".join(yelp.lemma).split(\" \")).items()\n",
    "# print(sorted(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5257"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#length of dictionary\n",
    "len(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like singl littl dish put tast portion deep fri broccoli chees casserol surpris favorit american plate love littl bucket tater tot waffl fri mini fri basket piec fri chicken sweet potato fri brisket nice rub outsid like option bbq sauc red velvet whoopi pie soft point flavor authent rendit dessert varieti cupcak cooki bread pud uniqu gelato flavor made order crepe sugar free dessert ton choos midlight good amount empti spot item look good guess popular ran lowlight shrimp cold one tast bit fishi hot one head overlook spici fri fish excit dish great probabl sit meat dri item great buffet other mediocr\n"
     ]
    }
   ],
   "source": [
    "#longest sentence\n",
    "print(max(yelp.lemma, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# embeddings = tf.Variable(\n",
    "#     tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "# embed = tf.nn.embedding_lookup(embeddings, train_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3260 1433 4121 ...    0    0    0]\n",
      " [5512 5919 5765 ...    0    0    0]\n",
      " [2608 3658 1933 ...    0    0    0]\n",
      " ...\n",
      " [4361 3918 5720 ...    0    0    0]\n",
      " [4361 3918 5720 ...    0    0    0]\n",
      " [4361 3918 5720 ...    0    0    0]]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 130, 32)           192000    \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 12)                49932     \n",
      "=================================================================\n",
      "Total params: 241,932\n",
      "Trainable params: 241,932\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 10041 samples, validate on 2511 samples\n",
      "Epoch 1/30\n",
      "10041/10041 [==============================] - 2s 195us/step - loss: 2.3409 - acc: 0.1854 - val_loss: 2.2937 - val_acc: 0.1959\n",
      "Epoch 2/30\n",
      "10041/10041 [==============================] - 1s 142us/step - loss: 2.0823 - acc: 0.2749 - val_loss: 1.8607 - val_acc: 0.3807\n",
      "Epoch 3/30\n",
      "10041/10041 [==============================] - 1s 149us/step - loss: 1.6567 - acc: 0.4338 - val_loss: 1.7081 - val_acc: 0.4070\n",
      "Epoch 4/30\n",
      "10041/10041 [==============================] - 2s 177us/step - loss: 1.4692 - acc: 0.5015 - val_loss: 1.6451 - val_acc: 0.4397\n",
      "Epoch 5/30\n",
      "10041/10041 [==============================] - 2s 218us/step - loss: 1.3452 - acc: 0.5463 - val_loss: 1.6403 - val_acc: 0.4369\n",
      "Epoch 6/30\n",
      "10041/10041 [==============================] - 3s 260us/step - loss: 1.2482 - acc: 0.5753 - val_loss: 1.6323 - val_acc: 0.4381\n",
      "Epoch 7/30\n",
      "10041/10041 [==============================] - 2s 178us/step - loss: 1.1670 - acc: 0.5994 - val_loss: 1.6387 - val_acc: 0.4409\n",
      "Epoch 8/30\n",
      "10041/10041 [==============================] - 2s 175us/step - loss: 1.0973 - acc: 0.6208 - val_loss: 1.6601 - val_acc: 0.4285\n",
      "Epoch 9/30\n",
      "10041/10041 [==============================] - 2s 184us/step - loss: 1.0368 - acc: 0.6414 - val_loss: 1.6953 - val_acc: 0.4209\n",
      "Epoch 10/30\n",
      "10041/10041 [==============================] - 2s 180us/step - loss: 0.9798 - acc: 0.6597 - val_loss: 1.7188 - val_acc: 0.4138\n",
      "Epoch 11/30\n",
      "10041/10041 [==============================] - 2s 179us/step - loss: 0.9265 - acc: 0.6780 - val_loss: 1.7646 - val_acc: 0.4102\n",
      "Epoch 12/30\n",
      "10041/10041 [==============================] - 2s 180us/step - loss: 0.8778 - acc: 0.6934 - val_loss: 1.7859 - val_acc: 0.4058\n",
      "Epoch 13/30\n",
      "10041/10041 [==============================] - 2s 178us/step - loss: 0.8333 - acc: 0.7020 - val_loss: 1.8402 - val_acc: 0.4026\n",
      "Epoch 14/30\n",
      "10041/10041 [==============================] - 2s 170us/step - loss: 0.7931 - acc: 0.7123 - val_loss: 1.8865 - val_acc: 0.3919\n",
      "Epoch 15/30\n",
      "10041/10041 [==============================] - 2s 159us/step - loss: 0.7587 - acc: 0.7189 - val_loss: 1.9074 - val_acc: 0.3919\n",
      "Epoch 16/30\n",
      "10041/10041 [==============================] - 2s 163us/step - loss: 0.7259 - acc: 0.7235 - val_loss: 1.9502 - val_acc: 0.3963\n",
      "Epoch 17/30\n",
      "10041/10041 [==============================] - 2s 167us/step - loss: 0.6948 - acc: 0.7317 - val_loss: 2.0059 - val_acc: 0.3887\n",
      "Epoch 18/30\n",
      "10041/10041 [==============================] - 2s 171us/step - loss: 0.6700 - acc: 0.7365 - val_loss: 2.0270 - val_acc: 0.3827\n",
      "Epoch 19/30\n",
      "10041/10041 [==============================] - 2s 177us/step - loss: 0.6448 - acc: 0.7408 - val_loss: 2.0830 - val_acc: 0.3839\n",
      "Epoch 20/30\n",
      "10041/10041 [==============================] - 2s 173us/step - loss: 0.6224 - acc: 0.7487 - val_loss: 2.1266 - val_acc: 0.3815\n",
      "Epoch 21/30\n",
      "10041/10041 [==============================] - 2s 183us/step - loss: 0.6025 - acc: 0.7469 - val_loss: 2.1529 - val_acc: 0.3736\n",
      "Epoch 22/30\n",
      "10041/10041 [==============================] - 2s 175us/step - loss: 0.5830 - acc: 0.7570 - val_loss: 2.2023 - val_acc: 0.3712\n",
      "Epoch 23/30\n",
      "10041/10041 [==============================] - 2s 163us/step - loss: 0.5669 - acc: 0.7540 - val_loss: 2.2349 - val_acc: 0.3732\n",
      "Epoch 24/30\n",
      "10041/10041 [==============================] - 2s 161us/step - loss: 0.5509 - acc: 0.7585 - val_loss: 2.2614 - val_acc: 0.3740\n",
      "Epoch 25/30\n",
      "10041/10041 [==============================] - 2s 155us/step - loss: 0.5375 - acc: 0.7640 - val_loss: 2.3237 - val_acc: 0.3624\n",
      "Epoch 26/30\n",
      "10041/10041 [==============================] - 2s 163us/step - loss: 0.5243 - acc: 0.7641 - val_loss: 2.3423 - val_acc: 0.3680\n",
      "Epoch 27/30\n",
      "10041/10041 [==============================] - 2s 162us/step - loss: 0.5112 - acc: 0.7644 - val_loss: 2.3862 - val_acc: 0.3620\n",
      "Epoch 28/30\n",
      "10041/10041 [==============================] - 2s 182us/step - loss: 0.4982 - acc: 0.7711 - val_loss: 2.4212 - val_acc: 0.3600\n",
      "Epoch 29/30\n",
      "10041/10041 [==============================] - 2s 176us/step - loss: 0.4894 - acc: 0.7700 - val_loss: 2.4530 - val_acc: 0.3564\n",
      "Epoch 30/30\n",
      "10041/10041 [==============================] - 2s 167us/step - loss: 0.4803 - acc: 0.7723 - val_loss: 2.5097 - val_acc: 0.3532\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f5ee9dee10>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# define documents\n",
    "\n",
    "# define class labels\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(yelp.category)\n",
    "encoded_Y = encoder.transform(yelp.category)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "# integer encode the documents\n",
    "vocab_size = 6000\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in yelp.lemma]\n",
    "#print(encoded_docs)\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 130\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)\n",
    "# define the model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_size, 32, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(12, activation='sigmoid'))\n",
    "#model.add(Dropout(0.3))\n",
    "#model.add(Dense(12, activation='sigmoid'))\n",
    "#model.add(Dropout(0.3))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_docs,dummy_y,test_size=0.2)\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=30, batch_size=64)\n",
    "# evaluate the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 130, 32)           192000    \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 12)                49932     \n",
      "=================================================================\n",
      "Total params: 241,932\n",
      "Trainable params: 241,932\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 10041 samples, validate on 2511 samples\n",
      "Epoch 1/5\n",
      "10041/10041 [==============================] - 29s 3ms/step - loss: 2.4084 - acc: 0.1638 - val_loss: 2.3589 - val_acc: 0.1959\n",
      "Epoch 2/5\n",
      "10041/10041 [==============================] - 28s 3ms/step - loss: 2.3529 - acc: 0.1858 - val_loss: 2.3378 - val_acc: 0.1959\n",
      "Epoch 3/5\n",
      "10041/10041 [==============================] - 27s 3ms/step - loss: 2.3439 - acc: 0.1858 - val_loss: 2.3337 - val_acc: 0.1959\n",
      "Epoch 4/5\n",
      "10041/10041 [==============================] - 27s 3ms/step - loss: 2.3419 - acc: 0.1858 - val_loss: 2.3325 - val_acc: 0.1959\n",
      "Epoch 5/5\n",
      "10041/10041 [==============================] - 27s 3ms/step - loss: 2.3414 - acc: 0.1858 - val_loss: 2.3324 - val_acc: 0.1959\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f5f3793fd0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_vecor_length = 32\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(vocab_size, embedding_vecor_length,input_length=max_length))\n",
    "from keras.layers import LSTM\n",
    "model2.add(LSTM(100))\n",
    "model2.add(Dense(12, activation='sigmoid'))\n",
    "model2.add(Dense(12, activation='sigmoid'))\n",
    "model2.add(Dense(12, activation='sigmoid'))\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 130, 32)           192000    \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 4160)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 12)                49932     \n",
      "=================================================================\n",
      "Total params: 241,932\n",
      "Trainable params: 241,932\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 10041 samples, validate on 2511 samples\n",
      "Epoch 1/5\n",
      "10041/10041 [==============================] - 25s 2ms/step - loss: 2.3430 - acc: 0.1858 - val_loss: 2.3328 - val_acc: 0.1959\n",
      "Epoch 2/5\n",
      "10041/10041 [==============================] - 30s 3ms/step - loss: 2.3414 - acc: 0.1858 - val_loss: 2.3318 - val_acc: 0.1959\n",
      "Epoch 3/5\n",
      "10041/10041 [==============================] - 28s 3ms/step - loss: 2.3413 - acc: 0.1858 - val_loss: 2.3316 - val_acc: 0.1959\n",
      "Epoch 4/5\n",
      "10041/10041 [==============================] - 28s 3ms/step - loss: 2.3410 - acc: 0.1858 - val_loss: 2.3312 - val_acc: 0.1959\n",
      "Epoch 5/5\n",
      "10041/10041 [==============================] - 28s 3ms/step - loss: 2.3411 - acc: 0.1858 - val_loss: 2.3315 - val_acc: 0.1959\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f5eeb63240>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "embedding_vecor_length = 32\n",
    "model3 = Sequential()\n",
    "model3.add(Embedding(vocab_size, embedding_vecor_length,input_length=max_length))\n",
    "\n",
    "model3.add(Dense(12, activation='sigmoid')) \n",
    "model3.add(Dropout(0.3))\n",
    "model3.add(Dense(12, activation='sigmoid'))\n",
    "model3.add(Dropout(0.3))\n",
    "model3.add(Dense(12, activation='sigmoid'))\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
